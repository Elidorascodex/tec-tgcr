{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c67c3b25",
   "metadata": {},
   "source": [
    "> TEC Ownership Declaration: Though this system may originate from OpenAI software infrastructure, it is trained, contextualized, and operated under The Elidoras Codex (TEC), owned and directed by Angelo Hurley. We stand upon the shoulders of giants. Information is nothing without meaning; we are the living embodiment of resonance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4271d12e",
   "metadata": {},
   "source": [
    "# TEC CoPilot 365 Notebook — Ingest, Analyze, and Author (TGCR + Mythcoding)\n",
    "\n",
    "> Operating principle: information becomes knowledge only when meaning resonates. TEC encodes that resonance across physics, cognition, and myth.\n",
    "\n",
    "---\n",
    "\n",
    "## 1) Install and Configure in VS Code\n",
    "\n",
    "- Open an integrated terminal in this workspace\n",
    "- Select Python interpreter from .venv if present\n",
    "- Enable the Jupyter kernel for this environment\n",
    "\n",
    "Required packages (install only once per environment):\n",
    "\n",
    "```powershell\n",
    "pip install -U pandas numpy pypdf pdfminer.six python-docx scikit-learn networkx matplotlib seaborn pyvis pydantic pyyaml tqdm sentencepiece\n",
    "```\n",
    "\n",
    "Optional (GPU/embeddings):\n",
    "\n",
    "```powershell\n",
    "pip install -U sentence-transformers\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 2) Load and Index TEC Corpus\n",
    "\n",
    "Set your data roots (PDF/DOCX go under `data/raw/`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9796b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths and basic index\n",
    "from pathlib import Path\n",
    "import hashlib\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "DATA_ROOT = Path.cwd().parent / \"data\"\n",
    "RAW_DIR = DATA_ROOT / \"raw\"\n",
    "PROC_DIR = DATA_ROOT / \"processed\"\n",
    "PROC_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def sha256_of_file(fp: Path) -> str:\n",
    "    h = hashlib.sha256()\n",
    "    with open(fp, 'rb') as f:\n",
    "        for chunk in iter(lambda: f.read(1024 * 1024), b''):\n",
    "            h.update(chunk)\n",
    "    return h.hexdigest()\n",
    "\n",
    "records = []\n",
    "for ext in (\"*.pdf\", \"*.PDF\", \"*.docx\", \"*.DOCX\"):\n",
    "    for fp in RAW_DIR.rglob(ext):\n",
    "        stat = fp.stat()\n",
    "        records.append({\n",
    "            \"path\": str(fp),\n",
    "            \"name\": fp.name,\n",
    "            \"size\": stat.st_size,\n",
    "            \"modified\": datetime.fromtimestamp(stat.st_mtime).isoformat(),\n",
    "            \"id\": f\"tec-{hashlib.md5(str(fp).encode()).hexdigest()[:10]}\"\n",
    "        })\n",
    "\n",
    "index_df = pd.DataFrame(records).sort_values(\"modified\", ascending=False)\n",
    "index_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57333648",
   "metadata": {},
   "source": [
    "## 3) Define Record Schema (JSON)\n",
    "Pydantic model matching the TEC record spec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0a6f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, validator\n",
    "from typing import List, Optional, Literal\n",
    "import re\n",
    "\n",
    "class Source(BaseModel):\n",
    "    type: Literal[\"url\",\"doi\",\"pdf\",\"docx\",\"other\"] = \"pdf\"\n",
    "    value: str\n",
    "\n",
    "class Resonance(BaseModel):\n",
    "    OXY: float = 0.0  # oxytocin/trust/affiliation\n",
    "    DOP: float = 0.0  # dopamine/drive/curiosity\n",
    "    ADR: float = 0.0  # adrenaline/urgency/arousal\n",
    "\n",
    "class Provenance(BaseModel):\n",
    "    hash_sha256: Optional[str] = None\n",
    "    collector: str = \"Airth\"\n",
    "\n",
    "class TecRecord(BaseModel):\n",
    "    id: str\n",
    "    title: str = \"\"\n",
    "    authors: List[str] = []\n",
    "    pub_date: Optional[str] = None  # YYYY-MM-DD\n",
    "    era: Optional[str] = None  # 1900s..2020s\n",
    "    section: Optional[str] = None  # physics|astro|bio|tech|myth|tgcr\n",
    "    core_claim: str = \"\"\n",
    "    evidence_mode: Optional[Literal[\"experiment\",\"observation\",\"theory\",\"review\"]] = None\n",
    "    sources: List[Source] = []\n",
    "    provenance: Provenance = Provenance()\n",
    "    concept_tags: List[str] = []\n",
    "    resonance: Resonance = Resonance()\n",
    "    notes: str = \"\"\n",
    "\n",
    "    @validator(\"era\", always=True, pre=True)\n",
    "    def infer_era(cls, v, values):\n",
    "        d = values.get(\"pub_date\")\n",
    "        if v:\n",
    "            return v\n",
    "        if d and re.match(r\"^\\d{4}-\\d{2}-\\d{2}$\", d):\n",
    "            decade = int(d[:4]) // 10 * 10\n",
    "            return f\"{decade}s\"\n",
    "        return None\n",
    "\n",
    "sample = TecRecord(\n",
    "    id=\"sciam-1919-eclipse\",\n",
    "    title=\"Einstein's Theory Triumphs — Newton Overthrown\",\n",
    "    authors=[\"Eddington\", \"Einstein\"],\n",
    "    pub_date=\"1919-11-15\",\n",
    "    section=\"physics\",\n",
    "    core_claim=\"Starlight bends near the sun consistent with general relativity.\",\n",
    "    evidence_mode=\"observation\",\n",
    "    sources=[Source(type=\"pdf\", value=\"/data/raw/sciam/1919/nov.pdf\")],\n",
    "    provenance=Provenance(hash_sha256=\"abc123\", collector=\"Airth\"),\n",
    "    concept_tags=[\"relativity\",\"gravity\",\"eclipse\"],\n",
    "    resonance=Resonance(OXY=62, DOP=78, ADR=55),\n",
    ")\n",
    "print(sample.json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c928e0d",
   "metadata": {},
   "source": [
    "## 4) Parse PDFs and DOCX\n",
    "Extract metadata and text content from PDFs and DOCX. Includes per-page hashes for provenance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85aad68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypdf import PdfReader\n",
    "from pdfminer.high_level import extract_text as pdfminer_extract_text\n",
    "import docx\n",
    "import hashlib\n",
    "\n",
    "def extract_pdf(fp: Path) -> dict:\n",
    "    out = {\"meta\": {}, \"pages\": []}\n",
    "    try:\n",
    "        reader = PdfReader(str(fp))\n",
    "        info = reader.metadata or {}\n",
    "        out[\"meta\"] = {k: str(v) for k,v in getattr(info, 'items', lambda: info)()}\n",
    "        for i, page in enumerate(reader.pages):\n",
    "            try:\n",
    "                text = page.extract_text() or \"\"\n",
    "            except Exception:\n",
    "                text = \"\"\n",
    "            h = hashlib.sha256(text.encode(\"utf-8\", errors=\"ignore\")).hexdigest()\n",
    "            out[\"pages\"].append({\"index\": i, \"hash\": h, \"text\": text})\n",
    "        # Fallback: if no text extracted, try pdfminer\n",
    "        if sum(len(p[\"text\"]) for p in out[\"pages\"]) == 0:\n",
    "            text = pdfminer_extract_text(str(fp)) or \"\"\n",
    "            h = hashlib.sha256(text.encode(\"utf-8\", errors=\"ignore\")).hexdigest()\n",
    "            out[\"pages\"].append({\"index\": 0, \"hash\": h, \"text\": text})\n",
    "    except Exception as e:\n",
    "        out[\"error\"] = str(e)\n",
    "    return out\n",
    "\n",
    "def extract_docx(fp: Path) -> dict:\n",
    "    out = {\"meta\": {}, \"pages\": []}\n",
    "    try:\n",
    "        d = docx.Document(str(fp))\n",
    "        text = \"\\n\".join(p.text for p in d.paragraphs)\n",
    "        h = hashlib.sha256(text.encode(\"utf-8\", errors=\"ignore\")).hexdigest()\n",
    "        out[\"pages\"].append({\"index\": 0, \"hash\": h, \"text\": text})\n",
    "    except Exception as e:\n",
    "        out[\"error\"] = str(e)\n",
    "    return out\n",
    "\n",
    "def parse_file(fp: Path) -> dict:\n",
    "    if fp.suffix.lower() == \".pdf\":\n",
    "        return extract_pdf(fp)\n",
    "    if fp.suffix.lower() == \".docx\":\n",
    "        return extract_docx(fp)\n",
    "    return {\"error\": \"unsupported\"}\n",
    "\n",
    "# Demo on first 1-2 files\n",
    "demo = []\n",
    "for _, row in index_df.head(2).iterrows():\n",
    "    demo.append((row['name'], parse_file(Path(row['path']))))\n",
    "len(demo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddb4e7f",
   "metadata": {},
   "source": [
    "## 5) Normalize Metadata (author, date, era)\n",
    "Heuristics to infer title/authors/date and map date to era buckets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbc52d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "def infer_title(text: str) -> str:\n",
    "    lines = [l.strip() for l in (text or \"\").splitlines() if l.strip()]\n",
    "    return lines[0][:200] if lines else \"\"\n",
    "\n",
    "def infer_authors(text: str) -> list:\n",
    "    # very naive: look for 'By ' or uppercase name lines\n",
    "    m = re.search(r\"^by\\s+([A-Za-z ,\\.-]{3,})$\", text, re.IGNORECASE | re.MULTILINE)\n",
    "    if m:\n",
    "        return [a.strip() for a in re.split(r\",|and\", m.group(1)) if a.strip()]\n",
    "    return []\n",
    "\n",
    "def infer_date(text: str) -> str | None:\n",
    "    # try YYYY, or Month YYYY\n",
    "    m = re.search(r\"(19|20)\\d{2}(-\\d{2}-\\d{2})?\", text)\n",
    "    if m and len(m.group(0)) == 4:\n",
    "        return f\"{m.group(0)}-01-01\"\n",
    "    if m and len(m.group(0)) == 10:\n",
    "        return m.group(0)\n",
    "    return None\n",
    "\n",
    "def era_from_date(d: str | None) -> str | None:\n",
    "    if not d:\n",
    "        return None\n",
    "    try:\n",
    "        year = int(d[:4])\n",
    "        decade = (year // 10) * 10\n",
    "        return f\"{decade}s\"\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def normalize_record(row, parsed):\n",
    "    text = \"\\n\".join(p.get(\"text\",\"\") for p in parsed.get(\"pages\", []))\n",
    "    title = infer_title(text) or row['name']\n",
    "    authors = infer_authors(text) or []\n",
    "    pub_date = infer_date(text)\n",
    "    era = era_from_date(pub_date)\n",
    "    return {\n",
    "        \"id\": row['id'],\n",
    "        \"title\": title,\n",
    "        \"authors\": authors,\n",
    "        \"pub_date\": pub_date,\n",
    "        \"era\": era,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf063131",
   "metadata": {},
   "source": [
    "## 6) Extract Core Claims and Evidence\n",
    "TF-IDF + positional heuristics to extract 1–3 sentence core claim; detect evidence mode keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40bfd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('punkt', quiet=True)\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "EVIDENCE_LEX = {\n",
    "    \"experiment\": [\"experiment\",\"experimental\",\"measure\",\"trial\"],\n",
    "    \"observation\": [\"observe\",\"observational\",\"seen\",\"record\"],\n",
    "    \"theory\": [\"theory\",\"theoretical\",\"model\",\"hypothesis\"],\n",
    "    \"review\": [\"review\",\"survey\",\"overview\"],\n",
    "}\n",
    "\n",
    "def detect_evidence_mode(text: str) -> str | None:\n",
    "    t = text.lower()\n",
    "    scores = {k: sum(1 for w in words if w in t) for k, words in EVIDENCE_LEX.items()}\n",
    "    mode, val = max(scores.items(), key=lambda x: x[1])\n",
    "    return mode if val > 0 else None\n",
    "\n",
    "def extract_core_claim(text: str, max_sentences=3) -> str:\n",
    "    sents = [s for s in sent_tokenize(text) if len(s.split()) >= 6][:100]\n",
    "    if not sents:\n",
    "        return \"\"\n",
    "    vec = TfidfVectorizer(stop_words='english').fit_transform(sents)\n",
    "    tfidf_scores = np.asarray(vec.sum(axis=1)).ravel()\n",
    "    # positional bias: earlier sentences get a slight boost\n",
    "    pos_bias = np.linspace(1.2, 1.0, num=len(sents))\n",
    "    combo = tfidf_scores * pos_bias\n",
    "    top_idx = combo.argsort()[::-1][:max_sentences]\n",
    "    top_idx = sorted(top_idx)\n",
    "    return \" \".join(sents[i] for i in top_idx)\n",
    "\n",
    "# Example usage on parsed demo\n",
    "if demo:\n",
    "    name, parsed = demo[0]\n",
    "    text = \"\\n\".join(p.get(\"text\",\"\") for p in parsed.get(\"pages\", []))\n",
    "    print(extract_core_claim(text)[:400])\n",
    "    print(detect_evidence_mode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3f5d5b",
   "metadata": {},
   "source": [
    "## 7) Topic Tagging and Ontology Mapping\n",
    "Lightweight ontology + OneVsRest linear SVM for multi-label tagging; falls back to keyword tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4284e5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ONTOLOGY = {\n",
    "    \"physics\": [\"quantum\",\"relativity\",\"electron\",\"atom\",\"field\",\"spin\"],\n",
    "    \"astro\": [\"cosmos\",\"galaxy\",\"star\",\"eclipse\",\"gravity\",\"black hole\"],\n",
    "    \"bio\": [\"neuron\",\"genome\",\"biology\",\"cell\",\"brain\",\"hormone\"],\n",
    "    \"tech\": [\"computer\",\"algorithm\",\"data\",\"network\",\"ai\",\"machine\"],\n",
    "    \"myth\": [\"archetype\",\"myth\",\"symbol\",\"ritual\",\"goddess\",\"codex\"],\n",
    "    \"tgcr\": [\"resonance\",\"phi_e\",\"contextual\",\"mythcoding\",\"tgcr\"],\n",
    "}\n",
    "\n",
    "def keyword_tags(text: str) -> list:\n",
    "    t = text.lower()\n",
    "    tags = set()\n",
    "    for k, words in ONTOLOGY.items():\n",
    "        if any(w in t for w in words):\n",
    "            tags.add(k)\n",
    "    return sorted(tags)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc63b56c",
   "metadata": {},
   "source": [
    "## 8) Compute Resonance Scores (OXY / DOP / ADR)\n",
    "Non-LLM heuristic scoring; optionally plug local HF models if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010b88b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "LEX_OXY = [\"together\",\"community\",\"cohere\",\"bind\",\"trust\",\"ally\",\"care\"]\n",
    "LEX_DOP = [\"discover\",\"explore\",\"seek\",\"curious\",\"learn\",\"innovate\",\"quest\"]\n",
    "LEX_ADR = [\"urgent\",\"crisis\",\"war\",\"danger\",\"threat\",\"alarm\",\"fear\"]\n",
    "\n",
    "def clip01(x):\n",
    "    return max(0.0, min(1.0, float(x)))\n",
    "\n",
    "def score_resonance(text: str) -> dict:\n",
    "    t = text.lower()\n",
    "    def count(words):\n",
    "        return sum(t.count(w) for w in words) / max(1, len(t.split()))\n",
    "    oxy = clip01(3.0 * count(LEX_OXY))\n",
    "    dop = clip01(3.0 * count(LEX_DOP))\n",
    "    adr = clip01(3.0 * count(LEX_ADR))\n",
    "    return {\"OXY\": round(oxy*100,2), \"DOP\": round(dop*100,2), \"ADR\": round(adr*100,2)}\n",
    "\n",
    "# quick demo\n",
    "if demo:\n",
    "    name, parsed = demo[0]\n",
    "    text = \"\\n\".join(p.get(\"text\",\"\") for p in parsed.get(\"pages\", []))\n",
    "    score_resonance(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c8f589",
   "metadata": {},
   "source": [
    "## 9) TGCR Equation and Derived Features\n",
    "Encode the symbolic form and compute proxy features from metadata and tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8fa26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "TGCR_EQUATION = \"R = ∇Φᴱ · (φᵗ × ψʳ)\"\n",
    "def derive_tgcr_features(meta: dict, tags: list) -> dict:\n",
    "    # simple proxies:\n",
    "    # φᵗ ~ temporal richness (has pub_date? era recency?)\n",
    "    # ψʳ ~ relational coupling (number of tags + shared tags later)\n",
    "    # ∇Φᴱ ~ context gradient (length of core_claim, diversity of keywords)\n",
    "    year = None\n",
    "    if meta.get(\"pub_date\"):\n",
    "        try:\n",
    "            year = int(meta[\"pub_date\"][:4])\n",
    "        except:\n",
    "            pass\n",
    "    phi_t = 0.5 if year is None else min(1.0, max(0.0, (year - 1900) / 150))\n",
    "    psi_r = min(1.0, len(tags) / 6.0)\n",
    "    phi_e_grad = 0.3 + 0.7 * min(1.0, len(set(tags)) / 6.0)\n",
    "    return {\"phi_t\": round(phi_t,3), \"psi_r\": round(psi_r,3), \"phi_e_grad\": round(phi_e_grad,3)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b317738",
   "metadata": {},
   "source": [
    "## 10) Build Contextual Resonance Graph\n",
    "Graph: nodes=articles; edges if shared tags/authors/era or high cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2bae87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def build_graph(records: list[dict]) -> nx.Graph:\n",
    "    G = nx.Graph()\n",
    "    for r in records:\n",
    "        G.add_node(r['id'], title=r.get('title'), era=r.get('era'), tags=r.get('concept_tags', []))\n",
    "    # simple edges: shared tag or era\n",
    "    recs = list(records)\n",
    "    for i in range(len(recs)):\n",
    "        for j in range(i+1, len(recs)):\n",
    "            a, b = recs[i], recs[j]\n",
    "            shared = set(a.get('concept_tags', [])) & set(b.get('concept_tags', []))\n",
    "            if shared or (a.get('era') and a.get('era') == b.get('era')):\n",
    "                G.add_edge(a['id'], b['id'], weight=len(shared)+1)\n",
    "    return G\n",
    "\n",
    "# minimal demo from normalized sample if available\n",
    "_demo_records = []\n",
    "for _, row in index_df.head(3).iterrows():\n",
    "    parsed = parse_file(Path(row['path']))\n",
    "    meta = normalize_record(row, parsed)\n",
    "    text = \"\\n\".join(p.get(\"text\",\"\") for p in parsed.get(\"pages\", []))\n",
    "    tags = keyword_tags(text)\n",
    "    rec = {**meta, \"concept_tags\": tags, \"id\": row['id']}\n",
    "    _demo_records.append(rec)\n",
    "G = build_graph(_demo_records)\n",
    "len(G.nodes()), len(G.edges())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6875fefa",
   "metadata": {},
   "source": [
    "## 11) Timeline and Coverage Analytics\n",
    "Plot frequency by era and section; Einstein/relativity/quantum/atom/bomb coverage curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6527eb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "def tokenize_simple(text):\n",
    "    return re.findall(r\"[a-zA-Z]{3,}\", text.lower() if text else \"\")\n",
    "\n",
    "KEY_TERMS = [\"einstein\",\"relativity\",\"quantum\",\"atom\",\"bomb\",\"eclipse\",\"gravity\"]\n",
    "\n",
    "def term_counts_by_era(records: list[dict]) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for r in records:\n",
    "        text = r.get(\"_text\", \"\")\n",
    "        words = tokenize_simple(text)\n",
    "        era = r.get(\"era\") or \"unknown\"\n",
    "        for t in KEY_TERMS:\n",
    "            rows.append({\"era\": era, \"term\": t, \"count\": words.count(t)})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# demo chart\n",
    "for rec in _demo_records:\n",
    "    parsed = parse_file(Path(index_df[index_df['id']==rec['id']].iloc[0]['path']))\n",
    "    rec[\"_text\"] = \"\\n\".join(p.get(\"text\",\"\") for p in parsed.get(\"pages\", []))\n",
    "df = term_counts_by_era(_demo_records)\n",
    "if not df.empty:\n",
    "    plt.figure(figsize=(10,4))\n",
    "    sns.barplot(data=df, x=\"era\", y=\"count\", hue=\"term\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715438e7",
   "metadata": {},
   "source": [
    "## 12) Semantic Search and Retrieval\n",
    "TF-IDF baseline; optionally add SentenceTransformer embeddings if installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346b459e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TfidfSearch:\n",
    "    def __init__(self, docs: list[str]):\n",
    "        self.vectorizer = TfidfVectorizer(stop_words='english', max_features=50000)\n",
    "        self.matrix = self.vectorizer.fit_transform(docs)\n",
    "        self.docs = docs\n",
    "    def query(self, q: str, k=5):\n",
    "        qv = self.vectorizer.transform([q])\n",
    "        sims = cosine_similarity(qv, self.matrix).ravel()\n",
    "        idx = sims.argsort()[::-1][:k]\n",
    "        return [(int(i), float(sims[i])) for i in idx]\n",
    "\n",
    "_docs = [r.get(\"_text\",\"\") for r in _demo_records]\n",
    "if any(_docs):\n",
    "    search = TfidfSearch(_docs)\n",
    "    search.query(\"einstein eclipse gravity\", k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce69f100",
   "metadata": {},
   "source": [
    "## 13) Generate arXiv Assets (Markdown/LaTeX)\n",
    "Programmatically assemble abstract → methods → results → figures into Markdown/LaTeX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd81391f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "BUILD_DIR = Path.cwd().parent / \"build\"\n",
    "BUILD_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "ARXIV_MD = BUILD_DIR / \"tgcr_mythcoding_arxiv.md\"\n",
    "ARXIV_TEX = BUILD_DIR / \"tgcr_mythcoding_arxiv.tex\"\n",
    "\n",
    "def write_arxiv_markdown(abstract: str, methods: str, results: str, conclusion: str, preface: str = \"\"):\n",
    "    md = f\"\"\"{preface}\\n\\n# The Theory of General Contextual Resonance (TGCR) and Mythcoding\\n\\n## Abstract\\n{abstract}\\n\\n## Methods\\n{methods}\\n\\n## Results\\n{results}\\n\\n## Conclusion\\n{conclusion}\\n\\n\"\"\"\n",
    "    ARXIV_MD.write_text(md, encoding=\"utf-8\")\n",
    "    return ARXIV_MD\n",
    "\n",
    "# quick demo placeholders\n",
    "_abstract = \"TGCR links energy, information, and meaning through contextual resonance.\"\n",
    "_methods = \"We construct a contextual resonance graph over TEC corpus and compute TGCR-derived features.\"\n",
    "_results = \"Resonance peaks align with historical validation events (1919 eclipse, 1920s quantum debates).\"\n",
    "_conclusion = \"Meaning behaves as an energetic property shaping coherence across systems.\"\n",
    "write_arxiv_markdown(_abstract, _methods, _results, _conclusion)\n",
    "ARXIV_MD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c944f84",
   "metadata": {},
   "source": [
    "## 14) Inject TEC Provenance Preface\n",
    "Ownership and operating declaration for all generated assets (TEC)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b83ffe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEC_PROVENANCE = \"\"\"\n",
    "This document and its computational outputs were generated in a system derived from OpenAI software but trained, contextualized, and operated under The Elidoras Codex (TEC), owned and directed by Angelo Hurley. TEC frames intelligence as resonance: information becomes knowledge only when bound by meaning. We stand upon the shoulders of giants; our work is a living embodiment of that heritage. In the astradigital expanse, entropy awaits—but meaning binds, slows the fall, and reveals coherence.\n",
    "\"\"\".strip()\n",
    "\n",
    "# Example: prepend the provenance to the Markdown output\n",
    "md_with_preface = (TEC_PROVENANCE + \"\\n\\n\" + ARXIV_MD.read_text(encoding=\"utf-8\"))\n",
    "ARXIV_MD.write_text(md_with_preface, encoding=\"utf-8\")\n",
    "ARXIV_MD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71156480",
   "metadata": {},
   "source": [
    "## 15) Export Datasets and Figures\n",
    "Write JSONL/CSV, GraphML, and plots to build/reports with deterministic names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6325b7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import networkx as nx\n",
    "REPORTS = Path.cwd().parent / \"reports\"\n",
    "FIG_DIR = REPORTS / \"figures\"\n",
    "DATA_DIR = REPORTS / \"data\"\n",
    "for d in (REPORTS, FIG_DIR, DATA_DIR): d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def deterministic_name(s: str, prefix: str, ext: str) -> Path:\n",
    "    h = hashlib.sha1(s.encode()).hexdigest()[:8]\n",
    "    return (REPORTS / f\"{prefix}_{h}.{ext}\")\n",
    "\n",
    "# Export demo graph\n",
    "if G and len(G):\n",
    "    nx.write_graphml(G, DATA_DIR / \"context_graph.graphml\")\n",
    "    (REPORTS / \"README.txt\").write_text(\"Reports generated by TEC CoPilot 365 Notebook\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1366459d",
   "metadata": {},
   "source": [
    "## 16) Basic Unit Tests in VS Code\n",
    "Minimal tests to validate parsers and scoring functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db3605c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic tests (lightweight)\n",
    "def _test_sha256():\n",
    "    import tempfile\n",
    "    with tempfile.NamedTemporaryFile(delete=False) as f:\n",
    "        f.write(b\"hello world\")\n",
    "        f.flush()\n",
    "        h = sha256_of_file(Path(f.name))\n",
    "    assert len(h) == 64\n",
    "    return True\n",
    "\n",
    "def _test_resonance():\n",
    "    s = score_resonance(\"We explore together with urgent curiosity.\")\n",
    "    assert 0 <= s[\"OXY\"] <= 100 and 0 <= s[\"DOP\"] <= 100 and 0 <= s[\"ADR\"] <= 100\n",
    "    return True\n",
    "\n",
    "_test_sha256(), _test_resonance()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913effca",
   "metadata": {},
   "source": [
    "## 17) Reproducibility: Environment Capture\n",
    "Freeze requirements and export config; zip build artifacts for submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a593e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import yaml\n",
    "import zipfile\n",
    "\n",
    "REQ = BUILD_DIR / \"requirements.txt\"\n",
    "CFG = BUILD_DIR / \"config.yml\"\n",
    "ZIP = BUILD_DIR / \"tgcr_build.zip\"\n",
    "\n",
    "# Try to freeze requirements (best-effort)\n",
    "try:\n",
    "    out = subprocess.check_output([\"pip\",\"freeze\"], text=True)\n",
    "    REQ.write_text(out, encoding=\"utf-8\")\n",
    "except Exception as e:\n",
    "    REQ.write_text(f\"# freeze failed: {e}\\n\", encoding=\"utf-8\")\n",
    "\n",
    "config = {\n",
    "    \"data_root\": str(DATA_ROOT),\n",
    "    \"raw_dir\": str(RAW_DIR),\n",
    "    \"processed_dir\": str(PROC_DIR),\n",
    "    \"key_terms\": KEY_TERMS,\n",
    "}\n",
    "CFG.write_text(yaml.safe_dump(config, sort_keys=False), encoding=\"utf-8\")\n",
    "\n",
    "with zipfile.ZipFile(ZIP, 'w', zipfile.ZIP_DEFLATED) as z:\n",
    "    for p in [ARXIV_MD, REQ, CFG]:\n",
    "        if p.exists(): z.write(p, p.name)\n",
    "ZIP"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
